# 2.1 Linear Regression

The first part of the second chapter of the book deals with linear regression with its two versions, Ordinary Least Square and Stochastic Gradient Descent. It also covers the associated loss function.

## Loss Function

_In statistics and machine learning, a loss function quantifies the losses generated by the errors that we commit when:_

- _we estimate the parameters of a statistical model;_
- _we use a predictive model, such as a linear regression, to predict a variable._

_The minimization of the expected loss, called statistical risk, is one of the guiding principles in statistical modelling._

> **Source:** https://www.statlect.com/glossary/loss-function

The MSE is a way to measure how well a linear regression model fits the data. It does this by calculating the average of the squares of the differences between the actual values and the predicted values.

### Example: Mean Square Error (MSE) for Linear Regression

\[
\text{MSE} = \frac{1}{n} \sum\_{i=1}^{n} (y_i - \hat{y}\_i)^2
\]

The Mean Squared Error (MSE) measures how well a linear regression model fits the data by following these steps:

1. **Calculate Errors**: For each data point, find the difference between the actual value and the predicted value. This is the error.

2. **Square the Errors**: Square these errors to make them positive and to give more weight to larger errors.

3. **Average**: Take the average of these squared errors. This average is the MSE, representing how well the model's predictions match the actual values. The lower the MSE, the better the model fits the data.

Here’s how you can write about **Ordinary Least Squares (OLS)** using `scikit-learn`, including its usage and some common use cases:

## Ordinary Least Squares (OLS)

Ordinary Least Squares (OLS) is a method used in linear regression to estimate the coefficients of the linear model. It minimizes the sum of the squared differences between the observed values and the values predicted by the linear model. This results in the "best fit" line through the data points.

## Usage

To implement OLS in Python using `scikit-learn`, you can use the `LinearRegression` class:

```python
from sklearn.linear_model import LinearRegression

# Example data
X = [[1], [2], [3], [4], [5]]  # Features (e.g., years of experience)
y = [1, 2, 3, 4, 5]            # Target values (e.g., salary)

# Initialize the model
model = LinearRegression()

# Fit the model
model.fit(X, y)

# Predict using the model
predictions = model.predict([[6], [7]])

print("Predicted values:", predictions)
```

## Use Cases

- **Predicting Housing Prices**: OLS can be used to predict house prices based on features like square footage, number of bedrooms, and location.

- **Salary Estimation**: Estimating salaries based on years of experience, education level, and job position.

- **Sales Forecasting**: Predicting future sales based on past sales data, marketing spend, and economic indicators.

- **Risk Assessment**: Estimating the risk of default on loans based on borrower characteristics like credit score and income.

- **Medical Research**: Analyzing the relationship between drug dosage and patient recovery time.

In all these cases, OLS helps find the linear relationship between the independent variables (features) and the dependent variable (target), allowing for accurate predictions and insights.

## Stochastic Gradient Descent (SGD)

Stochastic Gradient Descent (SGD) is an optimization method used to minimize the loss function in machine learning models, particularly in linear regression and classification tasks. Unlike batch gradient descent, which uses the entire dataset to compute the gradient, SGD updates the model's parameters using one or a few training examples at a time. This makes it faster and more efficient for large datasets.

## Usage

To implement SGD in Python using `scikit-learn`, you can use the `SGDRegressor` or `SGDClassifier` class, depending on your task. Here’s an example using `SGDRegressor` for a linear regression task:

```python
from sklearn.linear_model import SGDRegressor

# Example data
X = [[1], [2], [3], [4], [5]]  # Features (e.g., years of experience)
y = [1, 2, 3, 4, 5]            # Target values (e.g., salary)

# Initialize the model with SGD
model = SGDRegressor(max_iter=1000, tol=1e-3)

# Fit the model
model.fit(X, y)

# Predict using the model
predictions = model.predict([[6], [7]])

print("Predicted values:", predictions)
```

## Use Cases

- **Real-Time Prediction**: Due to its efficiency, SGD is commonly used in online learning settings where the model needs to be updated continuously as new data arrives.

- **Large-Scale Machine Learning**: SGD is ideal for training models on very large datasets because it processes one sample at a time, making it memory efficient.

- **Text Classification**: For tasks like spam detection or sentiment analysis, SGD can be used with models like logistic regression to classify large volumes of text data.

- **Recommendation Systems**: SGD is used in collaborative filtering algorithms to predict user preferences based on past behavior.

- **Deep Learning**: Although more advanced variants like Adam are often used, SGD remains a foundational optimization technique in training neural networks.

In these scenarios, SGD helps models converge faster, especially when dealing with large datasets or when quick updates are needed during the training process.
